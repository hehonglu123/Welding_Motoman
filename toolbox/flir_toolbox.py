import numpy as np
import cv2
##############################################FLIR COUNTS TO TEMPERATURE CONVERSION##############################################
####FROM https://flir.custhelp.com/app/answers/detail/a_id/3321/~/the-measurement-formula
def counts2temp(data_counts,R,B,F,J1,J0,Emiss):
    # reflected energy
    TRefl = 18

    # atmospheric attenuation
    TAtmC = 20
    TAtm = TAtmC + 273.15
    Tau = 0.99 #transmission

    # external optics
    TExtOptics = 20
    TransmissionExtOptics = 1
  
    K1 = 1 / (Tau * Emiss * TransmissionExtOptics)
        
    # Pseudo radiance of the reflected environment
    r1 = ((1-Emiss)/Emiss) * (R/(np.exp(B/TRefl)-F))
    # Pseudo radiance of the atmosphere
    r2 = ((1 - Tau)/(Emiss * Tau)) * (R/(np.exp(B/TAtm)-F)) 
    # Pseudo radiance of the external optics
    r3 = ((1-TransmissionExtOptics) / (Emiss * Tau * TransmissionExtOptics)) * (R/(np.exp(B/TExtOptics)-F))
            
    K2 = r1 + r2 + r3
    
    data_obj_signal = (data_counts - J0)/J1
    data_temp = (B / np.log(R/((K1 * data_obj_signal) - K2) + F)) -273.15
    
    return data_temp


def counts2temp_4learning(data_counts,R,B,F,J1,J0):
    Emiss=data_counts[-1]
    data_counts=data_counts[:-1]
    # reflected energy
    TRefl = 18

    # atmospheric attenuation
    TAtmC = 20
    TAtm = TAtmC + 273.15
    Tau = 0.99 #transmission

    # external optics
    TExtOptics = 20
    TransmissionExtOptics = 1
  
    K1 = 1 / (Tau * Emiss * TransmissionExtOptics)
        
    # Pseudo radiance of the reflected environment
    r1 = ((1-Emiss)/Emiss) * (R/(np.exp(B/TRefl)-F))
    # Pseudo radiance of the atmosphere
    r2 = ((1 - Tau)/(Emiss * Tau)) * (R/(np.exp(B/TAtm)-F)) 
    # Pseudo radiance of the external optics
    r3 = ((1-TransmissionExtOptics) / (Emiss * Tau * TransmissionExtOptics)) * (R/(np.exp(B/TExtOptics)-F))
            
    K2 = r1 + r2 + r3
    
    data_obj_signal = (data_counts - J0)/J1
    data_temp = (B / np.log(R/((K1 * data_obj_signal) - K2) + F)) -273.15
    
    return data_temp


def flame_detection(raw_img,threshold=1.2e4,area_threshold=10):
    ###flame detection by raw counts thresholding and connected components labeling
    #centroids: x,y
    #bbox: x,y,w,h
    thresholded_img=(raw_img>threshold).astype(np.uint8)
    if np.max(thresholded_img)==0:
        return None, None

    nb_components, labels, stats, centroids = cv2.connectedComponentsWithStats(thresholded_img, connectivity=4)

    valid_indices=np.where(stats[:, cv2.CC_STAT_AREA] > area_threshold)[0][1:]  ###threshold connected area
    if len(valid_indices)==0:
        return None, None
    
    average_pixel_values = [np.mean(raw_img[labels == label]) for label in valid_indices]   ###sorting

    valid_index=valid_indices[np.argmax(average_pixel_values)]      ###get the area with largest average brightness value

    # Extract the centroid and bounding box of the largest component
    centroid = centroids[valid_index]
    bbox = stats[valid_index, :-1]

    return centroid, bbox

def flame_detection_no_arc(raw_img,threshold=2.0e4,area_threshold=10):
    ###flame detection by raw counts thresholding and connected components labeling
    #centroids: x,y
    if np.max(raw_img)<threshold:
        return None
    #find the highest 10 pixel values and their x,y coordinates
    flat = raw_img.flatten()
    indices = np.argpartition(flat, -area_threshold)[-area_threshold:]
    
    # Convert the indices to x,y coordinates
    y, x = np.unravel_index(indices, raw_img.shape)
    
    # Get the corresponding values
    values = raw_img[y, x]
    
    # Combine the coordinates and values into a single list of tuples
    result = list(zip(values, zip(y, x)))
    
    # Sort the result by the values in descending order
    result.sort(key=lambda x: x[0], reverse=True)

    # Extract the coordinates from the result
    coordinates = np.array([coord for _, coord in result])

    # Calculate the mean of the coordinates
    centroid = np.mean(coordinates, axis=0)
    bbox = np.array([np.min(x),np.min(y),np.max(x)-np.min(x),np.max(y)-np.min(y)])

    return centroid, bbox


def weld_detection(raw_img,threshold=1.2e4,area_threshold=10):
    ###flame detection by raw counts thresholding and connected components labeling
    #centroids: x,y
    #bbox: x,y,w,h
    #pixels: row,col coordinates

    threshold=max(threshold,np.max(raw_img)*0.9)
    thresholded_img=(raw_img>threshold).astype(np.uint8)
    if np.max(thresholded_img)==0:
        # print('max counts below threshold: ',np.max(raw_img))
        return None, None, None

    nb_components, labels, stats, centroids = cv2.connectedComponentsWithStats(thresholded_img, connectivity=4)

    valid_indices=np.where(stats[:, cv2.CC_STAT_AREA] > area_threshold)[0][1:]  ###threshold connected area
    if len(valid_indices)==0:
        # print('no connected components')
        return None, None, None
    
    average_pixel_values = [np.mean(raw_img[labels == label]) for label in valid_indices]   ###sorting

    valid_index=valid_indices[np.argmax(average_pixel_values)]      ###get the area with largest average brightness value

    #list of pixel coordinates
    pixels=np.where(labels==valid_index)
    # Extract the centroid and bounding box of the largest component
    centroid = centroids[valid_index]
    bbox = stats[valid_index, :-1]

    return centroid, bbox, pixels

def torch_detect(ir_image,template):
    #threshold and normalize ir image
    ir_torch_tracking=ir_image.copy()
    ir_torch_tracking[ir_torch_tracking>1e4]=1e4
    ir_torch_tracking_normalized = ((ir_torch_tracking - np.min(ir_torch_tracking)) / (np.max(ir_torch_tracking) - np.min(ir_torch_tracking))) * 255

    # run edge detection
    edges = cv2.Canny(ir_torch_tracking_normalized.astype(np.uint8), threshold1=50, threshold2=200)
    # bolden all edges
    edges=cv2.dilate(edges,None,iterations=1)

    ###template matching with normalized image
    res = cv2.matchTemplate(edges,template,cv2.TM_CCOEFF_NORMED)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)
    return max_loc